import numpy as np
import gym
from gym import spaces
import random


class GoLeftEnv(gym.Env):
  """
  Custom Environment that follows gym interface.
  This is a simple env where the agent must learn to go always left. 
  """
  # Because of google colab, we cannot implement the GUI ('human' render mode)
  metadata = {'render.modes': ['console']}
  # Define constants for clearer code
  LEFT = 0
  RIGHT = 1

  def __init__(self, grid_size=10):
    super(GoLeftEnv, self).__init__()

    self.fields = [[0, 0, 3, 3,], [1, 0, 0, 0,], [2, 0, 0, 0,], [3, 0, 0, 0,], [4, 0, 0, 0,], [5, 0, 0, 1], [6, 0, 0, 0], [7, 0, 1, 0], [8, 0, 0, 1], [9, 0, 0, 0], [10, 0, 0, 0], [0, 0, 0, 0]]

    # Define action and observation space
    # They must be gym.spaces objects
    # Example when using discrete actions, we have two: left and right
    n_actions = 2
    self.action_space = spaces.Discrete(n_actions)
    # The observation will be the coordinate of the agent
    # this can be described both by Discrete and Box space
    self.observation_space = spaces.Box(low=-100, high=100,
                                        shape=(12,4), dtype=np.float32)

  def reset(self):
    """
    Important: the observation must be a numpy array
    :return: (np.array) 
    """

    # Initialize the agent at the right of the grid
    self.fields = [[0, 0, 3, 3,], [1, 0, 0, 0,], [2, 0, 0, 0,], [3, 0, 0, 0,], [4, 0, 0, 0,], [5, 0, 0, 1], [6, 0, 0, 0], [7, 0, 1, 0], [8, 0, 0, 1], [9, 0, 0, 0], [10, 0, 0, 0], [0, 0, 0, 0]]

    # here we convert to float32 to make it more general (in case we want to use continuous actions)
    return np.array([self.fields]).astype(np.float32)

  def step(self, action):
       
    balcycle1 = 0
    lst = [0, 1, 2, 3, 4]
    weights = [6.25, 25, 37.5, 25, 6.25]
    roll = random.choices(lst, weights=weights, k=1)
    roll = roll[0]
    self.fields[11][0] = roll
    print("Выпала" , roll)
    arrplayer1 = []
    while balcycle1 < 10:
     ballnow = self.fields[balcycle1][2]
     ballindex = self.fields[balcycle1][0]
#расположение фишек игрока 1
     if ballnow != 0:  
      arrplayer1.append(ballindex)    
     balcycle1 += 1

    print(arrplayer1) 

    balcycle1 = 0
    arrchoice1 = []
    while balcycle1 < 10:
     ballnow = self.fields[balcycle1][2]
     ballindex = self.fields[balcycle1][0]
     if ballnow != 0:  
       ballfuture = ballindex + roll
       if ballfuture <= 10:
        if ballfuture not in arrplayer1:
          arrchoice1.append(ballfuture)
      # print("Выпала" , self.roll)
          print("текущая позиция" , ballindex)
          print("возможная позиция" , ballfuture)   
     balcycle1 += 1

    print(arrchoice1)
    if arrchoice1:
     playeronemove = random.choice(arrchoice1)
     playeronepos = playeronemove - roll
     print(playeronemove)
     print(playeronepos)
     self.fields[playeronepos][2] = self.fields[playeronepos][2] - 1
     self.fields[playeronemove][2] = self.fields[playeronemove][2] + 1
     if self.fields[playeronemove][3] == 1 and playeronemove > 2 and playeronemove < 9:
        self.fields[playeronemove][3] = 0
        self.fields[0][3] = self.fields[0][3] + 1
        print("мы забрали шашку на" , self.fields[playeronemove][0])
     print(self.fields)

    agentcycle = 0
    arrplayeragent = []

    while agentcycle < 10:
     agentballnow = self.fields[agentcycle][3]
     agentballindex = self.fields[agentcycle][0]
#расположение фишек игрока 1
     if agentballnow != 0:  
      arrplayeragent.append(agentballindex)    
     agentcycle += 1

    print("фишки агента", arrplayeragent) 

    if action == self.LEFT:
      self.fields[10][3] -= 1
    elif action == self.RIGHT:
      self.fields[10][3] += 1
    else:
      raise ValueError("Received invalid action={} which is not part of the action space".format(action))

    # Account for the boundaries of the grid


    # Are we at the left of the grid?
    done = bool(self.fields[0][3] > 10)

    # Null reward everywhere except when reaching the goal (left of the grid)
    reward = 10 if self.fields[0][3] > 10 else 0

    # Optionally we can pass additional info, we are not using that for now
    info = {}

    return np.array([self.fields]).astype(np.float32), reward, done, info


  def render(self, mode='console'):
    if mode != 'console':
      raise NotImplementedError()
    # agent is represented as a cross, rest as a dot

    #print(self.fields[0])

  def close(self):
    pass
    
