from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy

# Instantiate the env
env = GoLeftEnv(grid_size=10)
# wrap it
env = make_vec_env(lambda: env, n_envs=1)

env.reset()
model_path = "logs/rl_model_500000_steps.zip"
model = PPO.load(model_path, env=env)

iii=0
iiia = []
while iii < 50:
  obs = env.reset()
  n_steps = 100
  for step in range(n_steps):
    action, _ = model.predict(obs, deterministic=True)
    print("Step {}".format(step + 1))
    print("Action: ", action)
    obs, reward, done, info = env.step(action)
    print('obs=', obs, 'reward=', reward, 'done=', done)
    if obs[0][4][2] == 1:
        print(u"\U0001F7E5", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][5][2] == 1:
        print(u"\U0001F7E5", end="")
    elif obs[0][5][3] == 1:
        print(u"\U0001F7E6", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][4][3] == 1:
        print(u"\U0001F7E6")
    else:
        print(u"\u2B1C")
    if obs[0][3][2] == 1:
        print(u"\U0001F7E5", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][6][2] == 1:
        print(u"\U0001F7E5", end="")
    elif obs[0][6][3] == 1:
        print(u"\U0001F7E6", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][3][3] == 1:
        print(u"\U0001F7E6")
    else:
        print(u"\u2B1C")
    if obs[0][2][2] == 1:
        print(u"\U0001F7E5", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][7][2] == 1:
        print(u"\U0001F7E5", end="")
    elif obs[0][7][3] == 1:
        print(u"\U0001F7E6", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][2][3] == 1:
        print(u"\U0001F7E6")
    else:
        print(u"\u2B1C")
    if obs[0][1][2] == 1:
        print(u"\U0001F7E5", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][8][2] == 1:
        print(u"\U0001F7E5", end="")
    elif obs[0][8][3] == 1:
        print(u"\U0001F7E6", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][1][3] == 1:
        print(u"\U0001F7E6")
    else:
        print(u"\u2B1C")
    print(u"\u2B1B", end="")
    if obs[0][9][2] == 1:
        print(u"\U0001F7E5", end="")
    elif obs[0][9][3] == 1:
        print(u"\U0001F7E6", end="")
    else:
        print(u"\u2B1C", end="")
    print(u"\u2B1B")
    print(u"\u2B1B", end="")
    if obs[0][10][2] == 1:
        print(u"\U0001F7E5", end="")
    elif obs[0][10][3] == 1:
        print(u"\U0001F7E6", end="")
    else:
        print(u"\u2B1C", end="")
    print(u"\u2B1B")
    if obs[0][14][2] == 1:
        print(u"\U0001F7E5", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][11][2] == 1:
        print(u"\U0001F7E5", end="")
    elif obs[0][11][3] == 1:
        print(u"\U0001F7E6", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][14][3] == 1:
        print(u"\U0001F7E6")
    else:
        print(u"\u2B1C")
    if obs[0][13][2] == 1:
        print(u"\U0001F7E5", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][12][2] == 1:
        print(u"\U0001F7E5", end="")
    elif obs[0][12][3] == 1:
        print(u"\U0001F7E6", end="")
    else:
        print(u"\u2B1C", end="")
    if obs[0][13][3] == 1:
        print(u"\U0001F7E6")
    else:
        print(u"\u2B1C")
    env.render(mode='console')
    if done:
      # Note that the VecEnv resets automatically
      # when a done signal is encountered
      iiia.append(reward)
      print("Goal reached!", "reward=", reward)
      break
  iii += 1
print(iiia)
win = []
notwin = []
for iiiaa in iiia:
  if iiiaa <= 0:
    notwin.append(iiiaa)
  elif iiiaa > 0:
    win.append(iiiaa)
print(win)
print(notwin)
print(len(win))
print(len(notwin))
# sample action:
print("sample action:", env.action_space.sample())

# observation space shape:
print("observation space shape:", env.observation_space.shape)

# sample observation:
print("sample observation:", env.observation_space.sample())
# Evaluate the trained agent
#mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, deterministic=True)

#print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")
